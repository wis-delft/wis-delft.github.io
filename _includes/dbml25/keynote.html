<section id="keynote" class="services-section section-lgrey">
    <div class="container">
        <div class="row text-left margin-top-2">
            <div class="col-md-12 text-center">
                <h2> SPEAKERS </h2>
            </div>
         </div>
         <div class="col-md-12 margin-top-10" id="keynote-renata">
                <div class="row pr-4 pl-5">
                    <div class="col-md-2">
                        <img src="{{site.baseurl}}/assets/img/dbml25/jiankang.png"
                             alt="Dr Jian kang" class="person-image-24">
                    </div>

                    <div class="col-md-8">
                        <h4 class="padding-bottom-1">On the Generalization of Temporal Graph Learning: Theoretical Insights and Simple Algorithms</h4>
                        <h5 class="padding-bottom-1">Jian Kang, University of Rochester</h5>
                        <p><strong>ABSTRACT.</strong> Real-world graphs often evolve over time. The de facto methods to extract spatial-temporal
                            information in temporal graphs are recurrent neural networks (RNNs) and self-attention mechanism (SAM). Despite
                            empirically good performance, their theoretical foundations largely remain uncharted. In this talk, I will share our
                            works on improving the generalization of temporal graph learning. I will first present a unified theoretical framework
                            to examine the generalization of three fundamental types of temporal graph learning methods. Guided by the theoretical
                            understanding, I will introduce two simple yet effective temporal graph learning models that enjoy small generalization
                            error, smooth optimization landscape and better empirical performance. We hope these results motivate researchers to
                            rethink the importance of simple model architectures.</p>

                        <p><strong>ABOUT.</strong> Jian Kang is an Assistant Professor in the Department of Computer Science at
                            the University of Rochester. His research focuses on machine learning on graphs to advance knowledge
                            understanding and scientific discovery. He received his Ph.D. in Computer Science from the University of Illinois
                            Urbana-Champaign. He was recognized as Rising Star in Data Science by The University of Chicago and a Mavis Future
                            Faculty Fellow by the University of Illinois Urbana-Champaign. He also serves as Guest Editor of Generative Search and
                            Recommendation of Frontiers in Big Data, Proceedings Chair of CIKM 2025, and Web Chair of KDD 2024.</p>
                    </div>
                </div>
            </div>
            <div class="col-md-12 margin-top-10" id="keynote-xiao">
                <div class="row pr-4 pl-5">
                    <div class="col-md-2">
                        <img src="{{site.baseurl}}/assets/img/dbml25/xiaohue.jpeg"
                             alt="Dr Xiaohui Yu" class="person-image-24">
                    </div>

                    <div class="col-md-8">
                        <h4 class="padding-bottom-1">Data Acquisition for Domain Adaptation of Closed-Box Models</h4>
                        <h5 class="padding-bottom-1">Contributing authors: Yiwei Liu, <b>Xiaohui Yu</b>, Nick Koudas </h5>
                        <p><strong>ABSTRACT.</strong> Machine learning (ML) marketplaces, pivotal for numerous industries,
                            often offer models to customers as "closed boxes". These models, when deployed in new domains,
                            might experience lower performance due to distributional shifts. Our paper proposes a framework
                            designed to enhance closed-box classification models. This framework allows customers, upon
                            detecting performance gaps on their validation datasets, to gather additional data for creating an
                            auxiliary "padding" model. This model assists the original closed-box model in addressing
                            classification weaknesses in the target domain. The framework includes a "weakness detector"
                            that identifies areas where the model falls short and an Augmented Ensemble method that combines
                            the original and padding models to improve accuracy and expand the diversity of the ML marketplace.
                            Extensive experiments on several popular benchmark datasets confirm the superiority of our proposed
                            framework over baseline approaches.</p>

                        <p><strong>ABOUT.</strong> Xiaohui Yu is a Professor and the Graduate Program Director in the School of
                            Information Technology, York University, Canada. He obtained his PhD degree from the University
                            of Toronto. His research interests lie in the broad area of data science, with a particular focus on
                            the intersection of data management and machine learning (ML). The results of his research have been
                            published in top data science journals and conferences, such as SIGMOD, VLDB, ICDE, and TKDE. He
                            regularly serves on the program committees of leading conferences and is an Associate/Area Editor
                            for the IEEE Transactions on Knowledge and Data Engineering (TKDE), the ACM Transactions on Knowledge
                            Discovery in Data (TKDD), and Information Systems. He is a General Co-Chair for the KDD 2025
                            conference. He has collaborated regularly with industry partners, and some research results have been
                            incorporated into large-scale production systems.</p>
                    </div>
                </div>
            </div>
            <div class="col-md-12 margin-top-10" id="keynote-essam">
                <div class="row pr-4 pl-5">
                    <div class="col-md-2">
                        <img src="{{site.baseurl}}/assets/img/dbml25/essammansour.png"
                             alt="Dr Essam Mansour" class="person-image-24">
                    </div>

                    <div class="col-md-8">
<!--                        <h4 class="padding-bottom-1">To be announced</h4>-->
                        <h5 class="padding-bottom-1">Essam Mansour , Concordia University</h5>
<!--                        <p><strong>ABSTRACT.</strong>To be announced</p>-->

<!--                        <p><strong>ABOUT.</strong>To do</p>-->
                    </div>
                </div>
            </div>
            <div class="col-md-12 margin-top-10" id="keynote-johannes">
                <div class="row pr-4 pl-5">
                    <div class="col-md-2">
                        <img src="{{site.baseurl}}/assets/img/dbml25/johanneswehrstein.png"
                             alt="Johannes Wehrstein" class="person-image-24">
                    </div>

                    <div class="col-md-8">
                        <h4 class="padding-bottom-1">Towards Foundation Database Models</h4>
                        <h5 class="padding-bottom-1">Johannes Wehrstein , TU Darmstadt</h5>
                        <p><strong>ABSTRACT.</strong> Recently, machine learning models have been utilized to realize many
                            database tasks in academia and industry. To solve such internal tasks of database systems, the
                            state-of-the-art is one-off models that need to be trained individually per task and even per
                            dataset, which causes extremely high training overheads. In this talk, we argue that a new learning
                            paradigm is needed that moves away from such one-off models towards generalizable models that can be
                            used with only minimal overhead for an unseen dataset on a wide spectrum of tasks. While recently,
                            several advances towards more generalizable models have been made, still, no model exists that can
                            generalize across both datasets and tasks. As such, we propose a new direction which we call
                            foundation models for databases, which is pre-trained in both task-agnostic and dataset-agnostic
                            manner, which makes it possible to use the model with low overhead to solve a wide spectrum of
                            downstream tasks on unseen datasets. In this vision talk, we propose an architecture for such a
                            foundation database model, describe a promising feasibility study with a first prototype of such a
                            model, and discuss the research roadmap to address the open challenges.</p>

                        <p><strong>ABOUT.</strong> Johannes Wehrstein is a Doctoral Researcher at the Systems Group @
                            TU Darmstadt, specializing in learned database components. His research focuses on leveraging AI to
                            enhance database performance, covering areas such as cost estimation, query optimization, advisory
                            systems, as well as foundational AI research on query-plan representation learning. Previously, he
                            was a Student Researcher at Systems Research @ Google, where he worked on foundation database
                            models — models that can generalize across tasks, workloads, and databases. He was recently awarded
                            the CIDR’25 best paper award for his work on foundation database models.</p>
                    </div>
                </div>
            </div>
            <div class="col-md-12 margin-top-10" id="keynote-dong">
                <div class="row pr-4 pl-5">
                    <div class="col-md-2">
                        <img src="{{site.baseurl}}/assets/img/dbml25/dongdeng.jpeg"
                             alt="Dong Deng" class="person-image-24">
                    </div>

                    <div class="col-md-8">
<!--                        <h4 class="padding-bottom-1">To be announced</h4>-->
                        <h5 class="padding-bottom-1">Dong Deng , Rutgers University</h5>
<!--                        <p><strong>ABSTRACT.</strong>To be announced</p>-->

<!--                        <p><strong>ABOUT.</strong>To do</p>-->
                    </div>
                </div>
            </div>
<!--            <div class="col-md-12 margin-top-10" id="keynote-fatemeh">-->
<!--                <div class="row pr-4 pl-5">-->
<!--                    <div class="col-md-2">-->
<!--                        <img src="{{site.baseurl}}/assets/img/dbml24/fatemeh.jpg"-->
<!--                             alt="Dr fatemeh Nargesian" class="person-image-24">-->
<!--                    </div>-->

<!--                    <div class="col-md-8">-->
<!--                        <h4 class="padding-bottom-1">Data Acquisition for AI</h4>-->
<!--                        <h5 class="padding-bottom-1">Fatemeh Nargesian, University of Rochester</h5>-->
<!--                        <p><strong>ABSTRACT.</strong> Data science is increasingly reliant on the discovery and integration of data from diverse sources such as open data portals and data marketplaces.-->
<!--                            With a massive collection of data like a data lake, dataset discovery involves searching for relevant datasets to downstream data science tasks.-->
<!--                            For multiple disjoint data sources, data acquisition streamlines the integration of sources to compile a dataset that meets specific schema and distribution requirements. -->
<!--                            In this talk, I will first describe how to develop efficient algorithms for dataset discovery and data enrichment based on the join operation. -->
<!--                            I will then present a method to construct a navigational structure over data lakes, offering an alternative discovery approach to the conventional keyword search. -->
<!--                            Next, we will see how to perform distribution-aware discovery in order to tailor a dataset with a desired distribution from multiple sources, aiming to address group representation issues. -->
<!--                            Finally, I will conclude with a discussion on the challenges of developing data acquisition systems that support AI-based analytics. -->
<!--                        <p><strong>ABOUT.</strong> Fatemeh Nargesian is an assistant professor of computer science at the University of Rochester. She obtained her PhD at the University of Toronto. -->
<!--                            Her research interests are in dataset discovery, distribution-aware data integration and data selection, and scientific time-series management. -->
<!--                            Her work has appeared at top-tier venues including VLDB, SIGMOD, and ICDE and received the best demo award of VLDB 2017.</p>-->
<!--                    </div>-->
<!--                </div>-->
<!--            </div>-->
            

         <!--   <div class="col-md-12 margin-top-10" id="keynote-immanuel">
                <div class="row pr-4 pl-5">
                    <div class="col-md-2">
                        <img src="{{site.baseurl}}/assets/img/dbml/itrummer.jpeg"
                             alt="Immanuel Trummer" class="person-image">
                    </div>

                    <div class="col-md-8">
                        <h4 class="padding-bottom-1">Towards AI-Generated Database Management Systems                        </h4>
                        <h5 class="padding-bottom-1">Immanuel Trummer, Cornell University</h5>
                        <p><strong>ABSTRACT.</strong> The past years have been marked by several breakthrough results in the domain of generative AI, 
                            culminating in the rise of tools like ChatGPT, able to solve a variety of language-related tasks without specialized training. 
                            In this talk, I outline novel opportunities in the context of data management, enabled by these advances. 
                            I discuss several recent research projects, aimed at exploiting advanced language processing for tasks such as 
                            parsing a database manual to support automated tuning, or mining data for patterns, described in natural language. Finally, 
                            I discuss our recent and ongoing research, aimed at synthesizing code for SQL processing in general-purpose programming languages, 
                            while enabling customization via natural language commands.
                        </p>

                        <p><strong>ABOUT.</strong> Immanuel Trummer is assistant professor at Cornell University. 
                            His papers were selected for “Best of VLDB”, “Best of SIGMOD”, for the ACM SIGMOD Research Highlight Award, 
                            and for publication in CACM as CACM Research Highlight. He received the NSF CAREER Award and multiple Google Faculty Research Awards.
                        </p>
                    </div>
                </div>
            </div>

           <div class="col-md-12 margin-top-10" id="keynote-ce">
                <div class="row pr-4 pl-5">
                    <div class="col-md-2">
                        <img src="{{site.baseurl}}/assets/img/dbml/ce_2022.png"
                             alt="Ce Zhang" class="person-image">
                    </div>

                    <div class="col-md-8">
                        <h4 class="padding-bottom-1">Optimizing Communications and Data for Large-scale Learning                        </h4>
                        <h5 class="padding-bottom-1">Ce Zhang, ETH Zurich</h5>
                        <p><strong>ABSTRACT.</strong> The rapid progress of machine learning in the last decade has been fueled by the increasing scale of data and compute. 
                            However, this ever-increasing scale has created significant challenges for machine learning, which center around two fundamental bottlenecks: 
                            data movement (communications) and data quality. To alleviate these bottlenecks, one must jointly optimize and analyze data and learning. 
                            In this talk, I will share some of our research in this direction, focusing on optimizing data movements to enable large-scale, distributed and decentralized learning.
                        </p>
                        <p><strong>ABOUT.</strong> Ce is an Associate Professor in Computer Science at ETH Zurich. 
                            The mission of his research is to make machine learning techniques widely accessible while being cost-efficient and trustworthy 
                            to everyone who wants to use them to make our world a better place. He believes in a system approach to enabling this goal, 
                            and his current research focuses on building next-generation machine learning platforms and systems that are data-centric, human-centric, and declaratively scalable.
                        </p>
                    </div>
                </div>
            </div>


            <div class="col-md-12 text-center margin-top-10">
                <h2> INVITED TALKS </h2>
            </div>

             <div class="col-md-12 margin-top-10" id="talk-mahmoud">
                <div class="row pr-4 pl-5">
                    <div class="col-md-2">
                        <img src="{{site.baseurl}}/assets/img/dbml/mahmoud.jpg"
                             alt="Mahmoud Abo Khamis" class="person-image">
                    </div>

                    <div class="col-md-8 ">
                        <h4 class="padding-bottom-1">Relational AutoDiff</h4>
                        <h5 class="padding-bottom-1">Mahmoud Abo Khamis, Senior Computer Scientist, RelationalAI</h5>
                        <p><strong>ABSTRACT.</strong> Modern database systems have been progressively expanding their use cases far outside traditional bookkeeping 
                            and data analytics, and into artificial intelligence workloads like machine learning and mathematical optimization. This in turn motivates 
                            the need for native in-database automatic differentiation to better support these use cases.

                            In this talk, we present RelationalAD (RAD), our framework for automatic differentiation at RelationalAI (RAI). 
                            Rel, the modeling language underlying RelationalAI, is declarative and can be viewed as a generalization of Datalog 
                            with infinite relations (e.g. arithmetic), aggregation, and relational abstraction. The input to RAD is a Rel program 
                            that defines a (set of) relational views and the output is another Rel program that defines new views that are the derivatives 
                            with respect to some given input relations. We show that performing AutoDiff inside a high-level database language like Rel 
                            allows us to evaluate derivatives while enjoying many features offered by the underlying database engine like factorization, 
                            query optimization and compilation, as well as support for higher-order derivatives. We present several examples covering 
                            recursive Rel programs, matrix calculus, neural network training, and gradient descent among others. We conclude with some challenges, 
                            design issues, as well as open problems.</p>
                        <p><strong>ABOUT.</strong> Mahmoud Abo Khamis is a Senior Computer Scientist at RelationalAI since 2017. 
                            He received his Ph.D. in Computer Science and Engineering from the State University of New York at Buffalo in 2016. 
                            He also worked as a Senior Database Engineer at Infor from 2015 until 2017. His research interests include database systems and theory, 
                            in-database machine learning, query optimization and evaluation, information theory, and beyond worst-case analysis. 
                            His work has received two PODS Best Paper Awards in 2016 and 2022, two SIGMOD Research Highlight Awards in 2016 and 2022, 
                            and the Best CSE Dissertation Award 2016 from SUNY Buffalo. His work also received several invitations to the Journal of the ACM, 
                            the ACM TODS, and the ACM STOC. He served on the program committees of PODS 2019, PODS 2021, and ICDT 2022, and he is also a reviewer 
                            for the VLDB Journal and the ACM TODS among others.
                        </p>
                    </div>
                </div>
            </div>


           <div class="col-md-12 margin-top-10" id="talk-feng">
                <div class="row pr-4 pl-5">
                    <div class="col-md-2">
                        <img src="{{site.baseurl}}/assets/img/dbml/FengZhang.jpg"
                             alt="FengZhang" class="person-image">
                    </div>

                    <div class="col-md-8">
                        <h4 class="padding-bottom-1">Applying Compressed Data Direct Computing from Database to ML Workloads</h4>
                        <h5 class="padding-bottom-1">Feng Zhang, Associate Professor, Renmin University, China</h5>
                        <p><strong>ABSTRACT.</strong> The rapid growth of data volume poses challenges for modern database systems 
                            in terms of space and time. Compressed data direct computing, as a solution that combines the advantages 
                            of space savings from data compression and efficiency gains from direct computing, has been proved to be 
                            a promising research in the database field. We find that the core of compressed data direct computing is 
                            data reuse, and it can be extended to ML workloads that are also concerned with data size and 
                            computational complexity. In this talk, we introduce Deep Reuse, a concrete implementation of data reuse 
                            into ML workloads. It shows great potential for inference latency reduction on popular neural networks 
                            such as CNNs. Inspired by Deep Reuse, we further carry out research in three aspects: 1) applying it to 
                            optimized operators such as the fast convolution algorithm Winograd, 2) embedding it into neural networks 
                            to address non-determinism and low accuracy problems through a consistent training process, and 
                            3) extending the application to resource-constrained IoT devices.</p>
                        <p><strong>ABOUT.</strong> Feng Zhang is an Associate Professor at Renmin University of China. 
                            He received his PhD from Tsinghua University in 2017, and has been a visiting scholar at NCSU in 2016 
                            and NUS in 2018. His research interests include databases and high-performance computing. 
                            He mainly studies high-performance direct computing on compression in data analytics and management. 
                            His papers are published in prestigious international conferences and journals including SIGMOD, VLDB, 
                            SC, USENIX ATC, ASPLOS, and NeurIPS. He got ACM SIGHPC China Rising Star Award and TPDS Best Paper Award. 
                            He has provided consulting services to numerous IT companies in China, including Alibaba, Tencent, 
                            and Ant Company.</p>
                    </div>
                </div>
            </div>

        </div> -->
        <!-- To be determined, it will be a group of very interesting people, so stay tuned. -->
    </div>
</section>